import os
import time
import numpy as np
from fastapi import FastAPI, HTTPException
from fastapi.responses import Response
from dotenv import load_dotenv

from prometheus_client import (
    Counter, Histogram, Gauge, generate_latest, REGISTRY
)

# ---- ê¸°ì¡´ Import ----
from models import TextInput, ResumeInput, MatchRequest, ScoreRequest, SummaryResponse, SkillsResponse, MatchResponse, SaveResumeResponse, ScoreResponse
from vector_store import create_index, save_resume, search_similar, get_resume
from llm import summarize, extract_skills
from scoring import rule_score

import ollama
from openai import OpenAI
import redis

load_dotenv()

tags_metadata = [
    {
        "name": "Summary",
        "description": "ì´ë ¥ì„œ/í…ìŠ¤íŠ¸ ìš”ì•½ì„ ìˆ˜í–‰í•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.",
    },
    {
        "name": "Skills",
        "description": "í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ  ìŠ¤íƒì„ ì¶”ì¶œí•˜ëŠ” ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤.",
    },
    {
        "name": "Matching",
        "description": "JD í…ìŠ¤íŠ¸ì™€ ì €ì¥ëœ ì´ë ¥ì„œë¥¼ ë²¡í„° ê¸°ë°˜ìœ¼ë¡œ ë§¤ì¹­í•©ë‹ˆë‹¤.",
    },
    {
        "name": "Scoring",
        "description": "JDì™€ í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ìˆ˜ì™€ ìƒì„¸ í‰ê°€ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.",
    },
    {
        "name": "Monitoring",
        "description": "Prometheusìš© ë©”íŠ¸ë¦­ ì—”ë“œí¬ì¸íŠ¸ì…ë‹ˆë‹¤. (Swaggerì—ëŠ” ìˆ¨ê¹€ ì²˜ë¦¬)",
    },
]

app = FastAPI(
    title="CoreBridge AI Matching Service",
    description="Ollama + Redis + Vector Store ê¸°ë°˜ì˜ ì´ë ¥ì„œ-JD ë§¤ì¹­/ìŠ¤ì½”ì–´ë§ ì„œë¹„ìŠ¤",
    version="1.0.0",
    openapi_tags=tags_metadata,
)

GEN_MODEL = os.getenv("GEN_MODEL", "llama3")
EMBEDDING_BACKEND = os.getenv("EMBEDDING_BACKEND", "ollama")
EMBEDDING_MODEL = os.getenv("EMBEDDING_MODEL", "nomic-embed-text")

redis_client = redis.Redis(host="localhost", port=6379, db=0)

openai_client = None
if EMBEDDING_BACKEND == "openai":
    openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ============================================
# ğŸ”¥ ê¸°ì¡´ ë©”íŠ¸ë¦­
# ============================================

REQUEST_COUNT = Counter(
    "ai_service_requests_total",
    "Total number of requests per endpoint",
    ["endpoint"]
)

REQUEST_LATENCY = Histogram(
    "ai_service_request_latency_seconds",
    "Latency of requests in seconds",
    ["endpoint"]
)

OLLAMA_LATENCY = Gauge(
    "ai_service_ollama_latency_ms",
    "Latency of Ollama processing"
)

EMBEDDING_LATENCY = Gauge(
    "ai_service_embedding_latency_ms",
    "Latency of embedding generation"
)

# ============================================
# ğŸ”¥ ì¶”ê°€ë˜ëŠ” íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­
# ============================================

SUMMARY_LAT = Gauge("ai_service_summary_latency_ms", "Summary latency")
SKILLS_LAT = Gauge("ai_service_skills_latency_ms", "Skills latency")
MATCH_LAT = Gauge("ai_service_match_latency_ms", "Match latency")
SCORE_LAT = Gauge("ai_service_score_latency_ms", "Score latency")

REDIS_LAT = Gauge("ai_service_redis_latency_ms", "Redis latency")

WORKFLOW_TOTAL = Gauge(
    "ai_workflow_total_processing_ms",
    "Entire n8n workflow total processing time"
)

ERROR_COUNT = Counter(
    "ai_service_errors_total",
    "Total errors in AI service",
    ["endpoint"]
)

# ============================================
# Utility: Embedding
# ============================================

def embed(text: str):
    start = time.time()
    try:
        if EMBEDDING_BACKEND == "ollama":
            resp = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)
            emb = resp["embedding"]
        else:
            resp = openai_client.embeddings.create(
                model=EMBEDDING_MODEL, input=text
            )
            emb = resp.data[0].embedding

        EMBEDDING_LATENCY.set((time.time() - start) * 1000)
        return emb
    except:
        ERROR_COUNT.labels(endpoint="embedding").inc()
        raise


def cosine(a: np.ndarray, b: np.ndarray) -> float:
    if a is None or b is None:
        return 0.0
    if a.ndim != 1:
        a = a.ravel()
    if b.ndim != 1:
        b = b.ravel()
    denom = (np.linalg.norm(a) * np.linalg.norm(b))
    if denom == 0:
        return 0.0
    return float(np.dot(a, b) / denom)

create_index()

# ============================================
# ğŸ”¥ Redis latency ì¸¡ì •ê¸°
# ============================================

def measure_redis_latency():
    t0 = time.time()
    redis_client.ping()
    REDIS_LAT.set((time.time() - t0) * 1000)

# ============================================
# ğŸ”¥ Endpoints
# ============================================

@app.post(
    "/summary",
    response_model=SummaryResponse,
    tags=["Summary"],
    summary="í…ìŠ¤íŠ¸ ìš”ì•½",
    description="ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ LLM(Ollama)ì„ ì‚¬ìš©í•´ í•œê¸€ ìš”ì•½ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.",
)
def api_summary(req: TextInput):
    endpoint = "/summary"
    REQUEST_COUNT.labels(endpoint).inc()
    measure_redis_latency()

    with REQUEST_LATENCY.labels(endpoint).time():
        t0 = time.time()
        try:
            result = summarize(req.text)
            lat = (time.time() - t0) * 1000
            SUMMARY_LAT.set(lat)
            OLLAMA_LATENCY.set(lat)
            return {"summary": result}
        except:
            ERROR_COUNT.labels(endpoint=endpoint).inc()
            raise


@app.post(
    "/skills",
    response_model=SkillsResponse,
    tags=["Skills"],
    summary="ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ",
    description="í…ìŠ¤íŠ¸(ì´ë ¥ì„œ, JD ë“±)ì—ì„œ ê¸°ìˆ  ìŠ¤íƒ/í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.",
)
def api_skills(req: TextInput):
    endpoint = "/skills"
    REQUEST_COUNT.labels(endpoint).inc()
    measure_redis_latency()

    with REQUEST_LATENCY.labels(endpoint).time():
        t0 = time.time()
        try:
            result = extract_skills(req.text)
            lat = (time.time() - t0) * 1000
            SKILLS_LAT.set(lat)
            OLLAMA_LATENCY.set(lat)
            return {"skills": result}
        except:
            ERROR_COUNT.labels(endpoint=endpoint).inc()
            raise


@app.post(
    "/match_jd",
    response_model=MatchResponse,
    tags=["Matching"],
)
def api_match(req: MatchRequest):
    endpoint = "/match_jd"
    REQUEST_COUNT.labels(endpoint).inc()
    measure_redis_latency()

    with REQUEST_LATENCY.labels(endpoint).time():
        t0 = time.time()
        try:
            jd_emb = embed(req.jd_text)
            hits = search_similar(jd_emb, k=req.top_k)

            # ---- ğŸ”¥ ì—¬ê¸°ì„œ í•„ë“œ ë³€í™˜ ----
            formatted = []
            for h in hits:
                formatted.append({
                    "candidate_id": h["key"].replace("candidate:", ""),
                    "score": h["score"]
                })

            MATCH_LAT.set((time.time() - t0) * 1000)

            return {"matches": formatted}

        except:
            ERROR_COUNT.labels(endpoint=endpoint).inc()
            raise


@app.post(
    "/save_resume",
    response_model=SaveResumeResponse,
    tags=["Matching"],
    summary="ì´ë ¥ì„œ ë²¡í„° ì €ì¥",
    description="í›„ë³´ìì˜ ì´ë ¥ì„œë¥¼ ì„ë² ë”© í›„ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•©ë‹ˆë‹¤.",
)
def api_save_resume(req: ResumeInput):
    endpoint = "/save_resume"
    REQUEST_COUNT.labels(endpoint).inc()
    measure_redis_latency()

    with REQUEST_LATENCY.labels(endpoint).time():
        try:
            # embedding ì²˜ë¦¬
            t0 = time.time()
            emb = embed(req.resume_text)
            lat = (time.time() - t0) * 1000

            # ì„ë² ë”© latencyëŠ” embed() ë‚´ë¶€ì—ì„œ ì´ë¯¸ ê¸°ë¡ë¨
            # ë³„ë„ latency ë©”íŠ¸ë¦­ì„ ë§Œë“¤ì§€ ì•Šì•„ë„ ë¨

            save_resume(req.candidate_id, emb, req.resume_text)

            return {
                "status": "saved",
                "candidate_id": req.candidate_id
            }

        except Exception:
            ERROR_COUNT.labels(endpoint=endpoint).inc()
            raise

@app.post(
    "/score",
    response_model=ScoreResponse,
    tags=["Scoring"],
    summary="í›„ë³´ì ì ìˆ˜ ê³„ì‚°",
    description="JDì™€ í›„ë³´ì ì´ë ¥ì„œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ë„/ìŠ¤í‚¬ ë§¤ì¹­ì„ ê³„ì‚°í•˜ê³  ìƒì„¸ ì ìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
)
def api_score(req: ScoreRequest):
    endpoint = "/score"
    REQUEST_COUNT.labels(endpoint).inc()
    measure_redis_latency()

    with REQUEST_LATENCY.labels(endpoint).time():
        t0 = time.time()
        try:
            cand = get_resume(req.candidate_id)
            if not cand:
                raise HTTPException(404, "candidate not found")

            cand_emb = cand["embedding"]
            cand_text = cand["resume_text"]

            jd_emb = embed(req.jd_text)
            cos = cosine(np.array(jd_emb, dtype=np.float32), cand_emb)

            jd_skills = req.required_skills or extract_skills(req.jd_text)
            cand_skills = extract_skills(cand_text)

            detail = rule_score(jd_skills, cand_skills, cos)

            SCORE_LAT.set((time.time() - t0) * 1000)
            return {
                "candidate_id": req.candidate_id,
                "required_skills": jd_skills,
                "candidate_skills": cand_skills,
                "cosine_similarity": round(cos, 4),
                "score_detail": detail
            }
        except:
            ERROR_COUNT.labels(endpoint=endpoint).inc()
            raise


# ============================================
# ğŸ”¥ metrics endpoint
# ============================================

@app.get(
    "/metrics",
    include_in_schema=False,     # â¬… Swagger(/docs)ì—ì„œ ìˆ¨ê¹€
)
def metrics():
    return Response(generate_latest(REGISTRY), media_type="text/plain")